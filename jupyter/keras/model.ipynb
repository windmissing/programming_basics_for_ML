{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用keras定义模型结构的两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(2))\n",
    "model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二\n",
    "\n",
    "layers = [keras.layers.LSTM(2), keras.layers.Dense(1)]\n",
    "model = keras.Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入的数据\n",
    "\n",
    "LSTM层要求的输入数据是三维的，分别代表样本数、时间步数、特征数  \n",
    "\n",
    "# 第一层\n",
    "\n",
    "第一层需要定义输入数据的形状  \n",
    "例如：  \n",
    "\n",
    "```python\n",
    "model.add(LSTM(5, input_shape(2, 1)))\n",
    "```\n",
    "\n",
    "`input_shape`的参数代表输入数据的时间步数和特征数。  \n",
    "默认的activation是linear  \n",
    "5是什么意思？  \n",
    "\n",
    "# 最后一层\n",
    "\n",
    "回归问题：不需要特殊定义的最后一层  \n",
    "二分类问题：`model.add(Activation('sigmoid'))`  \n",
    "多分类问题：`model.add(Activation('softmax'))`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[?]  \n",
    "默认情况下，LSTM的memory每个mini-batch会reset一次。  \n",
    "设置参数`stateful=True`以后就不会自动reset。由开发者手动reset。    \n",
    "以下情况会需要用到这一功能：  \n",
    "1. batch_size = 1  \n",
    "2. 一个长序列分成了几个子序列  \n",
    "3. 一个非常长的序列所以要考虑效率问题  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM作为输入层\n",
    "\n",
    "LSTM层要求的输入数据是三维的，分别代表样本数、时间步数、特征数。   \n",
    "对应的，LSTM层也要通过参数`input_shape`定义可以接受的输入数据的形状。input_shape的参数代表输入数据的时间步数和特征数。  \n",
    "默认的activation是linear。LSTM的第1个参数是输出向量的维度。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x636437750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "data = data.reshape(1, 10, 1)\n",
    "tf.keras.layers.LSTM(5, input_shape=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x636bc6d90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[1,2],[3,4],[5,6],[7,8],[9,10]])\n",
    "data = data.reshape(1, 5, 2)\n",
    "tf.keras.layers.LSTM(5, input_shape=(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM作为隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 10000\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个连续的LSTM层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 10000\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding是指把编码转成指定维度的向量，使具有相近语义的单词其向量也是相近的\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        # Flatten也可以用GlobalAveragePooling1D代替，后者速度更快\n",
    "        tf.keras.layers.Flatten().\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]   # embedding层\n",
    "weights = e.get_weights()[0]\n",
    "weights.shape()  # (10000, 16), 10000代表单词表的大小，16代表每个单词的维度\n",
    "# 单词序号转换为单词\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items])\n",
    "\n",
    "# meta.csv: 序号 --> 单词  \n",
    "# vecs.csv：序号 --> embedding\n",
    "import io\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size): # 0代表OOV\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + '\\n')\n",
    "    out_v.write('\\t'.join(str(x) for x in embeddings)+'\\n')\n",
    "out_m.close()\n",
    "out_v.close()\n",
    "# https://projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一维CNN做NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 10000\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如句子为：  \n",
    "[1, 2, 3, 4, 5, 6, 7]  \n",
    "[1, 3, 5, 7, 9]  \n",
    "[2, 4, 6, 8]  \n",
    "第一步：得到input_sequence为：  \n",
    "[1, 2]  \n",
    "[1, 2, 3]  \n",
    "[1, 2, 3, 4]  \n",
    "[1, 2, 3, 4, 5]  \n",
    "[1, 2, 3, 4, 5, 6]  \n",
    "[1, 2, 3, 4, 5, 6, 7]  \n",
    "[1, 3]  \n",
    "[1, 3, 5]  \n",
    "[1, 3, 5, 7]  \n",
    "[1, 3, 5, 7, 9]  \n",
    "[2, 4]  \n",
    "[2, 4, 6]  \n",
    "[2, 4, 6, 8]  \n",
    "第二步：以前面补0的方式全部对齐到同样的长度（长度为7）  \n",
    "第三步：每一行的前6个为输入，最后一个为输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '.......'  # 训练数据\n",
    "corpus = data.lower().split('\\n')\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1 # +1代表OOV\n",
    "\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequence.append(n_gram_sequence)\n",
    "        \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences,\n",
    "                                         maxlen = max_sequences_len,\n",
    "                                         padding='pre'))\n",
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes = totoal_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 另一种写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_VariantDataset' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2c1a40283680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwindow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_VariantDataset' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "for val in dataset:\n",
    "    print (val.numpy())\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "for window in dataset:\n",
    "    print (window.numpy())\n",
    "dataset = dataset.map(lambda window:window[:-1], window[-1:])\n",
    "dataset = dataset.shuffle(buffer_size=10)\n",
    "dataset = dataset.batch(2).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "l0 = tf.keras.layers.Dense(1, input_shape=[window_size])\n",
    "model = tf.keras.models.Sequential([l0])\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
    "model.fit(dataset, epochs=100, verbose=0)\n",
    "model.predict(series[1:21][np.newaxis])\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch:1e-8 * 10**(epoch/20)\n",
    ")\n",
    "history = model.fit(dataset, epochs, callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到下图的结果，选择底部平坦处中间位置的点作为真正的lr\n",
    "![](assets/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    # 第二层RNN，如果没有`return_sequences=True`,只在最后一个时间步有输出。\n",
    "    # 如果有`return_sequences=True`，则每个时间步有输出，就是seq2seq模型\n",
    "    tf.keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Lambda at 0x274f77e9e80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 这句话放在keras的第一层，相当于数据的预处理\n",
    "# input_shape=[None]代表输入为任意数据\n",
    "tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None])\n",
    "\n",
    "# 这句话放在最后一层，相当于输出数据的后期处理\n",
    "tf.keras.layers.Lambda(lambda x: x*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "models = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filter=32, kernel_size=5, strides=5, padding='causal', activation='relu', input_shape=['None', 1])\n",
    "    tf.keras.layers.LSTM(32, return_sequences = True),\n",
    "    tf.keras.layers.LSTM(32, return_sequences = True),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    tf.keras.layers.Lambda(lambda x:x*20)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
